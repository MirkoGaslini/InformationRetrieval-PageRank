{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Libraries importing:\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from operator import add\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "#Opening PySpark Session:\n",
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "#Defining computeContribs function:\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "#Defining parseNeighbors function:\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(\"\t\")\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "#Defining Diff function between 2 lists:\n",
    "def Diff(li1, li2):\n",
    "    return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))\n",
    "\n",
    "#Defining intersection function between 2 lists:\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "#Reading all the lines of Google web graph: \n",
    "lines = sc.textFile('web-Google.txt')\n",
    "print(\"Reading and skipping\")\n",
    "lines = lines.zipWithIndex().filter(lambda tup: tup[1] > 3).map(lambda tup: tup[0])\n",
    "links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "# Find node count\n",
    "N = links.count()\n",
    "print(N)\n",
    "ranks = links.map(lambda url_tuple: (url_tuple[0], 1.0))\n",
    "\n",
    "old_ranks = ranks\n",
    "delta = 1\n",
    "\n",
    "temp_struct=ranks.map(lambda tupla : tupla[0])\n",
    "temp_struct_collect=temp_struct.collect()\n",
    "itPR = 0\n",
    "\n",
    "#PageRank algorithm.\n",
    "print(\"Starting PageRank... \")\n",
    "while(delta > 1.0e-6):\n",
    "    print(\"qui\")\n",
    "    itPR = itPR + 1\n",
    "    contribs = links.join(old_ranks).flatMap(lambda tupla : computeContribs(tupla[1][0],tupla[1][1]))\n",
    "    dict_contr=contribs.collectAsMap()\n",
    "    temp_struct_current=list(dict_contr.keys())\n",
    "    ris=Diff(temp_struct_collect, temp_struct_current)\n",
    "    val=intersection(ris, temp_struct_collect)\n",
    "    result = map(lambda e: (e,0),val) \n",
    "    result=list(result)\n",
    "    result=sc.parallelize(result)\n",
    "    contribs=contribs.union(result)\n",
    "    n_ranks = contribs.reduceByKey(add).mapValues(lambda rank: (rank * 0.90) + 0.10)\n",
    "    if(itPR!=1):\n",
    "        n_ranks_df = pd.DataFrame(n_ranks.sortByKey().collect(), columns =['Node', 'Score'])\n",
    "        old_ranks_df = pd.DataFrame(old_ranks.sortByKey().collect(), columns =['Node', 'Score'])\n",
    "        df1 = abs(n_ranks_df['Score'].sub(old_ranks_df['Score'],axis=0))\n",
    "        delta=df1.sum()\n",
    "    old_ranks = n_ranks\n",
    "    n_ranks=None\n",
    "    n_ranks_df=None\n",
    "    old_ranks_df=None\n",
    "    del contribs\n",
    "    print(\"Delta: \" , delta)\n",
    "print(\"Finish PageRank... \")\n",
    "print(\"Number of iterations: \", itPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation in df to save in CSV\n",
    "old_ranks_df = pd.DataFrame(old_ranks.sortByKey().collect(), columns =['Node', 'Score'])\n",
    "old_ranks_df.sort_values(by=['Score'], inplace=True, ascending=False)\n",
    "\n",
    "# Write CSV \n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pandas import DataFrame\n",
    "\n",
    "old_ranks_df.to_csv(path_or_buf=\"csv_PR_0.10_v3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving PR dataframe\n",
    "import pickle\n",
    "with open('dataframePR_0.10_v3.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(old_ranks_df,  f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from graphframes import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlContext.createDataFrame(pandas_df, p_schema)\n",
    "\n",
    "links = lines.map(lambda urls: parseNeighbors(urls))\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this row only if you use the saved resut \n",
    "old_ranks_df=old_ranks_df.rename(columns={\"Node\":\"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices= pandas_to_spark(old_ranks_df)\n",
    "edges = sqlContext.createDataFrame(links, [\"src\", \"dst\"])\n",
    "\n",
    "#graph graphframes\n",
    "from graphframes import *\n",
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis connected components\n",
    "sc.setCheckpointDir(\"/tmp/graphframes-example-connected-components\")\n",
    "result = g.connectedComponents()\n",
    "\n",
    "#grouping and counting all the connected components\n",
    "import pyspark.sql.functions as f\n",
    "sorted_connected=result.groupBy('component').count().select('component', f.col('count').alias('n')).orderBy('n', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#community detection\n",
    "result2 = g.labelPropagation(maxIter=3)\n",
    "result2_df = pd.DataFrame(result2.collect(), columns =['id', 'label', 'score'])\n",
    "\n",
    "#saving result community \n",
    "with open('result2_v3.pkl', 'wb') as f:\n",
    "     pickle.dump(result2_df,  f)\n",
    "     \n",
    "#Write CSV \n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pandas import DataFrame\n",
    "\n",
    "result2_df.to_csv(path_or_buf=\"result2_df_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each label (community) counting how many nodes\n",
    "import pyspark.sql.functions as f\n",
    "sorted_l=result2.groupBy('label').count().select('label', f.col('count').alias('n')).orderBy('n', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I check which node belongs to the node to choose the community \n",
    "result2.filter(result2.id ==  41909).show()#661424963782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection of community  \n",
    "component_selected=result2.filter(result2.label == 661424963782 )\n",
    "component_selected=component_selected.select(\"id\")\n",
    "\n",
    "#community's vertices  \n",
    "result_app=result2.select(\"id\",\"label\")\n",
    "vertices_sub_graph = vertices.join(result_app, vertices.id ==result_app.id,how=\"left\").drop(result_app.id)\n",
    "vertices_sub_graph = vertices_sub_graph.filter(vertices_sub_graph.label  ==661424963782) \n",
    "\n",
    "#community's edges \n",
    "edges_sub_graph = edges.join(result_app, edges.src ==result_app.id,how=\"left\").drop(result_app.id)\n",
    "edges_sub_graph = edges_sub_graph.withColumnRenamed(\"label\", \"labelsrc\")\n",
    "edges_sub_graph = edges_sub_graph.join(result_app, edges_sub_graph.dst == result_app.id,how=\"left\").drop(result_app.id)\n",
    "edges_sub_graph=edges_sub_graph.filter( (edges_sub_graph.label  == 661424963782) | (edges_sub_graph.label  == 661424963782) )\n",
    "edges_sub_graph=edges_sub_graph.drop(edges_sub_graph.labelsrc)\n",
    "edges_sub_graph=edges_sub_graph.drop(edges_sub_graph.label)\n",
    "\n",
    "def listOfTuples(l1, l2): \n",
    "    return list(map(lambda x, y:(x,y), l1, l2)) \n",
    "\n",
    "edges_sub_graph_src=list(edges_sub_graph.select('src').toPandas()['src'])\n",
    "edges_sub_graph_dst=list(edges_sub_graph.select('dst').toPandas()['dst']) \n",
    "edges_sub_graph_list_tuple=listOfTuples(edges_sub_graph_src, edges_sub_graph_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list creartioon of vertices for networkX\n",
    "list_app=[]\n",
    "for vertex in vertices_sub_graph.collect():\n",
    "    my_dict = {\n",
    "      \"Score\": vertex.Score ,\n",
    "    }\n",
    "    my_tupla=(vertex.id,my_dict)\n",
    "    list_app.append(my_tupla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list ofvertices \n",
    "list_vertices=list_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation graph networkX for Cytoscape \n",
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(list_vertices)\n",
    "G.add_edges_from(edges_sub_graph_list_tuple)\n",
    "\n",
    "#centrality measures \n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "nx.set_node_attributes(G, betweenness_centrality, \"betweenness\")\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "nx.set_node_attributes(G, closeness_centrality, \"closeness\")\n",
    "degreee_centrality = nx.degree_centrality(G)\n",
    "nx.set_node_attributes(G, degreee_centrality, \"degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_gml(\"test.gml\")\n",
    "h,a=nx.hits(G, max_iter=100, tol=1e-06, nstart=None, normalized=True)#hubs and authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, h, name=\"Hubs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, a, name=\"Authorities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(G, \"test.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file for Cytoscape \n",
    "nx.write_gml(G, \"test.gml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
